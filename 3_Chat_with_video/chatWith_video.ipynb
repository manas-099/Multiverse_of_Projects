{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxzBACxQr_ZQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsEXJ0eexOWI",
        "outputId": "5ab77d97-0da9-48ed-f15f-defd0e540047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q moviepy langchain  faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtHKpUPLySoH"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GdFxEqher8bi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"]=\"\"\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsdakitPnfFt",
        "outputId": "ad064e42-abc3-41ad-b966-e083943a60e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[youtube] Extracting URL: https://www.youtube.com/watch?v=hmqYxByTlRs\n",
            "[youtube] hmqYxByTlRs: Downloading webpage\n",
            "[youtube] hmqYxByTlRs: Downloading tv simply player API JSON\n",
            "[youtube] hmqYxByTlRs: Downloading tv client config\n",
            "[youtube] hmqYxByTlRs: Downloading player 6742b2b9-main\n",
            "[youtube] hmqYxByTlRs: Downloading tv player API JSON\n",
            "[info] hmqYxByTlRs: Downloading 1 format(s): 247+251\n",
            "[download] Destination: Create Simple RAG based AI Chat bot ｜ Python+LangChain+FAISS Vector Database(No API Keys Needed!).f247.webm\n",
            "[download] 100% of    7.91MiB in 00:00:00 at 9.20MiB/s   \n",
            "[download] Destination: Create Simple RAG based AI Chat bot ｜ Python+LangChain+FAISS Vector Database(No API Keys Needed!).f251.webm\n",
            "[download] 100% of    6.56MiB in 00:00:01 at 6.02MiB/s   \n",
            "[Merger] Merging formats into \"Create Simple RAG based AI Chat bot ｜ Python+LangChain+FAISS Vector Database(No API Keys Needed!).webm\"\n",
            "Deleting original file Create Simple RAG based AI Chat bot ｜ Python+LangChain+FAISS Vector Database(No API Keys Needed!).f251.webm (pass -k to keep)\n",
            "Deleting original file Create Simple RAG based AI Chat bot ｜ Python+LangChain+FAISS Vector Database(No API Keys Needed!).f247.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q yt-dlp\n",
        "\n",
        "# Example Python code to download video\n",
        "import yt_dlp\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=hmqYxByTlRs\"\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo+bestaudio/best',\n",
        "    'outtmpl': '%(title)s.%(ext)s'\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PeRhoQU8mXuz"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "\n",
        "def download_youtube_video(url):\n",
        "    \"\"\"\n",
        "    Download a YouTube video with user-selected resolution.\n",
        "    Falls back to highest available if requested resolution is not present.\n",
        "    \"\"\"\n",
        "    # Ask user for preferred resolution\n",
        "    resolution = input(\"Enter desired resolution (e.g., 1080, 720, 480): \")\n",
        "\n",
        "    # yt-dlp format string: video + audio best matching resolution\n",
        "    ydl_opts = {\n",
        "        'format': f'bestvideo[height<={resolution}]+bestaudio/best',\n",
        "        'outtmpl': '%(title)s.%(ext)s',\n",
        "        'merge_output_format': 'mp4'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "        print(\"Successfully downloaded!\")\n",
        "    except Exception as e:\n",
        "        print(\"Error downloading video:\", e)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JQuufsBpCEt",
        "outputId": "572bcc32-59d5-4880-a2cc-d80987d74deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter desired resolution (e.g., 1080, 720, 480): 480\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=wrpmqMz-m4w\n",
            "[youtube] wrpmqMz-m4w: Downloading webpage\n",
            "[youtube] wrpmqMz-m4w: Downloading tv simply player API JSON\n",
            "[youtube] wrpmqMz-m4w: Downloading tv client config\n",
            "[youtube] wrpmqMz-m4w: Downloading tv player API JSON\n",
            "[info] wrpmqMz-m4w: Downloading 1 format(s): 397+251\n",
            "[download] Destination: This 2 Minute Challenge can make you a Topper⚡.f397.mp4\n",
            "[download] 100% of    3.60MiB in 00:00:00 at 4.70MiB/s   \n",
            "[download] Destination: This 2 Minute Challenge can make you a Topper⚡.f251.webm\n",
            "[download] 100% of    1.98MiB in 00:00:00 at 25.11MiB/s  \n",
            "[Merger] Merging formats into \"This 2 Minute Challenge can make you a Topper⚡.mp4\"\n",
            "Deleting original file This 2 Minute Challenge can make you a Topper⚡.f397.mp4 (pass -k to keep)\n",
            "Deleting original file This 2 Minute Challenge can make you a Topper⚡.f251.webm (pass -k to keep)\n",
            "Successfully downloaded!\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.youtube.com/watch?v=wrpmqMz-m4w\"\n",
        "download_youtube_video(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoT4U10xsd8H"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zTZBboZpvXqR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "\n",
        "# Function to chunk video\n",
        "def chunk_video(video_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits video into chunks of `chunk_length` seconds.\n",
        "    Returns list of video clip objects.\n",
        "    \"\"\"\n",
        "    clip = VideoFileClip(video_path)\n",
        "    duration = int(clip.duration)\n",
        "    chunks = []\n",
        "    for start in range(0, duration, chunk_length):\n",
        "        end = min(start + chunk_length, duration)\n",
        "        chunk = clip.subclip(start, end)\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Function to summarize a video chunk\n",
        "def summarize_chunk(chunk, chunk_index):\n",
        "    \"\"\"\n",
        "    Converts video chunk to text summary using LLM.\n",
        "    \"\"\"\n",
        "    # Export chunk as temporary file\n",
        "    temp_path = f\"temp_chunk_{chunk_index}.mp4\"\n",
        "    chunk.write_videofile(temp_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
        "\n",
        "    # Encode chunk to base64\n",
        "    import base64\n",
        "    with open(temp_path, \"rb\") as f:\n",
        "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    # Create LLM message\n",
        "    message = HumanMessage(\n",
        "        content=[\n",
        "            {\"type\": \"text\", \"text\": \"Summarize the content of this video segment.\"},\n",
        "            {\"type\": \"media\", \"data\": encoded, \"mime_type\": \"video/mp4\"},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    # Cleanup temp file\n",
        "    os.remove(temp_path)\n",
        "    return response.content  #  ChatGoogleGenerativeAI style response\n",
        "\n",
        "# Function to process full video and build vector store\n",
        "def ingest_video(video_path, chunk_length=30):\n",
        "    chunks = chunk_video(video_path, chunk_length)\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "        summary = summarize_chunk(chunk, i)\n",
        "        summaries.append(summary)\n",
        "    print(\"All chunks summarized!\")\n",
        "\n",
        "    # Create vector store\n",
        "    # Define the model parameters\n",
        "    model_name = \"BAAI/bge-small-en\"\n",
        "    model_kwargs = {\"device\": \"cpu\"}\n",
        "    encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "    hf_em_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "\n",
        "    vector_store = FAISS.from_texts(summaries, hf_em_model)\n",
        "    return vector_store\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471,
          "referenced_widgets": [
            "66869f2c048e4fab9e45c06b0dfa4dc4",
            "5a067a5d3da6402783f29a4dcfcea6a6",
            "4b9919de18324a008a10eb3c55acf3fc",
            "39d94ad0b31a4fedbe804fef16d2ee33",
            "4be7489c6fd04f38b73cf686b0259c09",
            "de70fdc3841140b68f4984a83128653f",
            "e2d3017af2d24c28bd4048a2b4034328",
            "1d63b7891e184db88e2a10ca13a89f3d",
            "728bfa7ca68b47dd92b75ff776202e3c",
            "9a24ac8112d84a2184c5bbb504e418a1",
            "d22fb41eeeaf423abbb97d4c804efe4c"
          ]
        },
        "id": "yicb8-Hw39K9",
        "outputId": "cf8b21dc-3a5c-4633-aec2-a08cb2affc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarizing chunk 1/4...\n",
            "Summarizing chunk 2/4...\n",
            "Summarizing chunk 3/4...\n",
            "Summarizing chunk 4/4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/tmp/ipython-input-4200157175.py:69: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  hf_em_model = HuggingFaceBgeEmbeddings(\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All chunks summarized!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66869f2c048e4fab9e45c06b0dfa4dc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a067a5d3da6402783f29a4dcfcea6a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b9919de18324a008a10eb3c55acf3fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39d94ad0b31a4fedbe804fef16d2ee33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4be7489c6fd04f38b73cf686b0259c09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de70fdc3841140b68f4984a83128653f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2d3017af2d24c28bd4048a2b4034328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d63b7891e184db88e2a10ca13a89f3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "728bfa7ca68b47dd92b75ff776202e3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a24ac8112d84a2184c5bbb504e418a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d22fb41eeeaf423abbb97d4c804efe4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "video_file = \"/content/This 2 Minute Challenge can make you a Topper⚡.mp4\"\n",
        "vector_store = ingest_video(video_file, chunk_length=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3k08dV_D1Pkl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template=\"\"\"\n",
        "you are a helpfull assistance.\n",
        "generate answer to user query based on the context provided\n",
        "here is the user query:{query}\n",
        "here is the context :{context}\n",
        "\"\"\"\n",
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "rag_chain=prompt|llm|StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aUWsgsIHyiUS"
      },
      "outputs": [],
      "source": [
        "# Query the video\n",
        "query = \"What happens in the last part of the video?\"\n",
        "docs = vector_store.similarity_search(query)\n",
        "response=rag_chain.invoke({\"query\":query,\"context\":docs})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9oFFrRck5Oe8",
        "outputId": "236e9fab-034d-46a9-a9e3-5dd82fe3004f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In the last part of the video, the speaker encourages viewers to accept a challenge to study for one hour right after they wake up for the next six days. He ends the video by saying, \"I want you to win, and I believe in you.\"'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fwUOwPf85QDG",
        "outputId": "5a661d4e-0050-4a0d-ed84-601e3faacc40"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In the middle part of the video, the speaker encourages viewers to accept a challenge to study for two minutes every day for six days. The challenge is to plan out what you are going to study early in the morning right after you wake up. The subject you choose should not be your favorite. After six days, try to analyze which of the subjects you were able to study the best in the morning.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what heppens in the middle part of the video\"\n",
        "docs = vector_store.similarity_search(query)\n",
        "response=rag_chain.invoke({\"query\":query,\"context\":docs})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J07jHYHl5fPP",
        "outputId": "e6bfad8a-aab5-4442-b8bc-b20b073079ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, but based on the provided text, the speaker's YouTube channel name is not mentioned.\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"does he mention his youtube channel name if yes what is it?\"\n",
        "docs = vector_store.similarity_search(query)\n",
        "response=rag_chain.invoke({\"query\":query,\"context\":docs})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xO01xIDo5lAT",
        "outputId": "b104a790-e818-440e-be98-2cd3b90528cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The book mentioned in the video is called \"When.\"'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"which book was mentioned in this video?\"\n",
        "docs = vector_store.similarity_search(query)\n",
        "response=rag_chain.invoke({\"query\":query,\"context\":docs})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp7qCRyC8pgW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
